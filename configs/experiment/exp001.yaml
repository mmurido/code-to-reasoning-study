# @package _global_

defaults:
  - /model: pythia-1b
  - /peft: lora-pythia
  - /dataset: starcoderdata-multi
  - /training: base
  - /eval: base
  - _self_

run:
  id: pythia-1b_lora__starcoderdata-multi
  timestamp: null

training:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  max_steps: 2000

eval:
  tasks:
    - gsm8k_cot
    - anli
    - hellaswag
    - bbh_cot_fewshot_logical_deduction_three_objects
    - bbh_cot_fewshot_boolean_expressions
    - boolq
    - arc_easy
    - piqa
